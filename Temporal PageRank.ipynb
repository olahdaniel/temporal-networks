{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal PageRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import operator\n",
    "import copy\n",
    "from datetime import datetime, timedelta\n",
    "import os.path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beolvasás"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# numberOfEdges: 0 - all; n - first n edge\n",
    "def readRealGraph(filepath, numberOfEdges = 0):\n",
    "    edgesTS = []\n",
    "    nodes = set()\n",
    "    edges = set()\n",
    "    lookup = {}          # szótár, kulcsok az élek lesznek, ezekhez rendel egy id-t\n",
    "    c = 0\n",
    "    with open(filepath,'r') as fd:\n",
    "        for line in fd.readlines():\n",
    "\n",
    "            line = line.strip()\n",
    "            items = line.split(' ')\n",
    "            tstamp = ' '.join(items[0:2])\n",
    "            tstamp = tstamp[1:-1]\n",
    "            tstamp = datetime.strptime(tstamp, '%Y-%m-%d %H:%M:%S')\n",
    "            t = items[2:4]\n",
    "            t = list(map(int,t))\n",
    "            if t[0] == t[1]:\n",
    "                continue\n",
    "            #t.sort(); #undirected\n",
    "\n",
    "            if tuple(t) in lookup.keys():\n",
    "                num = lookup[tuple(t)]\n",
    "            else:\n",
    "                num = c\n",
    "                lookup[tuple(t)] = c\n",
    "                c += 1\n",
    "            edgesTS.append((tstamp, tuple(t), num ))\n",
    "            nodes.add(t[0])\n",
    "            nodes.add(t[1])\n",
    "            edges.add(tuple([t[0],t[1]]))\n",
    "    fd.close()\n",
    "    if 0 < numberOfEdges and numberOfEdges < len(edgesTS):\n",
    "        return edgesTS[0:numberOfEdges], nodes, edges\n",
    "    return edgesTS, nodes, edges\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### gráf csinálás"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def first_n_edge(filepath, n):\n",
    "    G = nx.DiGraph()\n",
    "    edges_df = pd.read_csv(filepath, sep=' ', names=[\"time\",\"source\",\"target\"])\n",
    "    for i in iter(range(n)):\n",
    "        G.add_edge(edges_df.ix[i][\"source\"],edges_df.ix[i][\"target\"],{'time':edges_df.ix[i][\"time\"]})\n",
    "        \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getSubgraph(G, N = 1000):\n",
    "    Gcc = sorted(nx.connected_component_subgraphs(G.to_undirected()), key = len, reverse=True)\n",
    "    print (len(Gcc))\n",
    "    nodes = set()\n",
    "    i = 0\n",
    "\n",
    "    while len(nodes) < N:\n",
    "        s = np.random.choice(Gcc[i].nodes())\n",
    "        i += 1\n",
    "        nodes.add(s)\n",
    "        for edge in nx.bfs_edges(G.to_undirected(), s):\n",
    "            nodes.add(edge[1])\n",
    "            if len(nodes) == N:\n",
    "                break\n",
    "    return nx.subgraph(G, nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getGraph(edgesTS):\n",
    "    G = nx.DiGraph()\n",
    "    edges = {}\n",
    "\n",
    "    for item in edgesTS:\n",
    "        edge = item[1]\n",
    "        edges[edge] = edges.get(edge, 0.0) + 1.0\n",
    "\n",
    "    #nrm = float(sum(edges.values()))\n",
    "    G.add_edges_from([(k[0],k[1], {'weight': v}) for k,v in edges.items()])\n",
    "    #G.add_edges_from([tuple(edge)])\n",
    "    return G\n",
    "\n",
    "\n",
    "def weighted_DiGraph(sampling_mode=True, numberOfEdges = 0):\n",
    "    file_name = os.path.join('..',\"temporal-pagerank\",\"Data\",\"facebook.txt\")\n",
    "    if sampling_mode:\n",
    "        edgesTS, _, _ = readRealGraph(file_name, numberOfEdges)\n",
    "        G = getGraph(edgesTS)\n",
    "        G = nx.DiGraph(G)\n",
    "        G.remove_edges_from(G.selfloop_edges())\n",
    "#        G = getSubgraph(G, 100)\n",
    "    else:\n",
    "        G = first_n_edge(file_name,100)\n",
    "        \n",
    "    for i in G.nodes_iter():\n",
    "        if G.out_degree(i) == 0:\n",
    "            for j in G.nodes_iter():\n",
    "                if i != j:\n",
    "                    G.add_edge(i, j, weight=1.0)\n",
    "\n",
    "#    print (nx.info(G))\n",
    "\n",
    "#    w = 1.0/G.number_of_edges()\n",
    "#    for i in G.edges_iter():\n",
    "#        G[i[0]][i[1]]['weight'] = w\n",
    "        \n",
    "    nrm = float(sum(G.out_degree(weight = 'weight').values()))\n",
    "    for i in G.edges_iter(data=True):\n",
    "        G[i[0]][i[1]]['weight'] = i[-1]['weight']/nrm\n",
    "        \n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.path.join('.','..',\"temporal-pagerank\",\"Data\",\"facebook.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "face_graph = weighted_DiGraph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# nx.pagerank(face_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (nx.info(face_graph))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal PR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def flowPR(p_prime_nodes, ref_pr, stream, RS, current, iters = 1000000, alpha = 0.85, beta=0.001, gamma=0.9999, normalization = 1.0, padding = 0):\n",
    "    \"\"\" current: s\n",
    "        RS: r\n",
    "        p_prime_node: h*/h'\n",
    "        \"\"\"\n",
    "    if beta == 1.0:\n",
    "        beta = 0.0\n",
    "        \n",
    "    tau = []\n",
    "    pearson = []\n",
    "    spearman = []\n",
    "    error = []\n",
    "    x = []\n",
    "    i = 0\n",
    "\n",
    "    rank_order = [key for (key, value) in sorted(ref_pr.items(), key=operator.itemgetter(1), reverse=True)]\n",
    "    ordered_pr = np.array([ref_pr[k] for k in rank_order])\n",
    "\n",
    "    for e in stream:\n",
    "        i += 1\n",
    "\n",
    "        RS[e[0]] = RS.get(e[0], 0.0) * gamma + 1.0 * (1.0 - alpha) * p_prime_nodes[e[0]] * normalization\n",
    "        RS[e[1]] = RS.get(e[1], 0.0) * gamma + (current.get(e[0], 0.0) + 1.0 * (1.0 - alpha) * p_prime_nodes[e[0]]) * alpha * normalization\n",
    "        current[e[1]] = current.get(e[1], 0.0) + (current.get(e[0], 0.0) + 1.0 * (1.0 - alpha)* p_prime_nodes[e[0]]) * alpha *(1 - beta)\n",
    "        current[e[0]] = current.get(e[0], 0.0) * beta\n",
    "\n",
    "\n",
    "        if (i % 100 == 0 or i == len(stream)) and len(RS) == len(ordered_pr):\n",
    "            if i == iters-1:\n",
    "                print (sum(RS.values()))\n",
    "            sorted_RS4 = np.array([RS[k] / sum(RS.values()) for k in rank_order])\n",
    "            tau.append(scipy.stats.kendalltau(sorted_RS4, ordered_pr)[0])\n",
    "            pearson.append(scipy.stats.pearsonr(sorted_RS4, ordered_pr)[0])\n",
    "            spearman.append(scipy.stats.spearmanr(sorted_RS4, ordered_pr)[0])\n",
    "            error.append(np.linalg.norm(sorted_RS4 - ordered_pr))\n",
    "            x.append(i+padding)\n",
    "\n",
    "        if i == iters-1:\n",
    "            print (sum(RS.values()))\n",
    "\n",
    "    sorted_RS4 = np.array([RS[k] / sum(RS.values()) for k in rank_order])\n",
    "\n",
    "    return RS, current, tau, spearman, pearson, error, x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Számolás egy 100 csúcsú részgráfra (facebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "G = weighted_DiGraph(sampling_mode = True, numberOfEdges = 1000)\n",
    "norm = sum(G.out_degree(weight='weight').values())\n",
    "sampling_edges = {e[:-1]: e[-1]['weight']/norm for e in G.edges_iter(data=True)}\n",
    "stream = list(sampling_edges.keys())\n",
    "\n",
    "# basic (degree personalization)\n",
    "personalization = {k: v / norm for k, v in G.out_degree(weight='weight').items()}\n",
    "p_prime_nodes = {i: personalization[i]/G.out_degree(i, weight='weight') for i in G.nodes_iter()}\n",
    "pr_basic = nx.pagerank(G, personalization=personalization, weight='weight')\n",
    "RS4_basic, current_basic = {}, {}\n",
    "RS4_basic, current_basic, tau_basic, spearman_basic, pearson_basic, error_basic, x = flowPR(p_prime_nodes, pr_basic, stream, RS4_basic, current_basic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RS4_basic, current_basic, tau_basic, spearman_basic, pearson_basic, error_basic, x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "import numpy as np\n",
    "import operator\n",
    "\n",
    "def get_binned_values(ordered_pr_ref, ordered_pr_out, bins_num):\n",
    "    bins = -np.log(np.linspace(min(ordered_pr_ref + ordered_pr_out), max(ordered_pr_ref + ordered_pr_out), num = bins_num))\n",
    "    ranking_ref = np.digitize(-np.log(ordered_pr_ref), bins)\n",
    "    ranking_out = np.digitize(-np.log(ordered_pr_out), bins)\n",
    "    return ranking_ref, ranking_out\n",
    "\n",
    "\n",
    "def get_ordered(pr_ref, pr_out):\n",
    "    rank_order = [key for (key, value) in sorted(pr_ref.items(), key=operator.itemgetter(1), reverse=True)]\n",
    "    ordered_pr_ref = np.array([pr_ref[k] for k in rank_order])\n",
    "    ordered_pr_out = np.array([pr_out[k]/sum(pr_out.values()) for k in rank_order])\n",
    "    return ordered_pr_ref, ordered_pr_out\n",
    "\n",
    "def get_ordered_ranks(pr_ref, pr_out):\n",
    "    rank_order = [key for (key, value) in sorted(pr_ref.items(), key=operator.itemgetter(1), reverse=True)]\n",
    "    ordered_pr_ref = np.array([1.0 - pr_ref[k] for k in rank_order])\n",
    "    ordered_pr_out = np.array([1.0 - pr_out[k]/sum(pr_out.values()) for k in rank_order])\n",
    "    #ordered_pr_out__ = np.array([pr_out[k]/sum(pr_out.values()) for k in rank_order])\n",
    "    out_rank = scipy.stats.rankdata(ordered_pr_out)\n",
    "    ref_rank = scipy.stats.rankdata(ordered_pr_ref)\n",
    "    return ref_rank, out_rank\n",
    "\n",
    "def get_topk_corr(pr_ref, pr_out, k_range, bins = 0):\n",
    "\n",
    "    ordered_pr_ref, ordered_pr_out = get_ordered(pr_ref, pr_out)\n",
    "\n",
    "    if bins > 0:\n",
    "        ref_rank, out_rank = get_binned_values(ordered_pr_ref, ordered_pr_out, bins)\n",
    "    else:\n",
    "        ref_rank = scipy.stats.rankdata(ordered_pr_ref)\n",
    "        out_rank = scipy.stats.rankdata(ordered_pr_out)\n",
    "\n",
    "    spearman_top = []\n",
    "    tau_top = []\n",
    "    for k in k_range:\n",
    "        tau_top.append(scipy.stats.kendalltau(out_rank[:k], ref_rank[:k])[0])\n",
    "        spearman_top.append(scipy.stats.spearmanr(out_rank[:k], ref_rank[:k])[0])\n",
    "    return tau_top, spearman_top\n",
    "\n",
    "def get_topk_corr_union(pr_ref, pr_out, k_range, bins = 0):\n",
    "\n",
    "    pr_out = {k:v/sum(pr_out.values()) for (k,v) in pr_out.iteritems()}\n",
    "    sorted_ref = sorted(pr_ref.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    sorted_out = sorted(pr_out.items(), key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "    if bins > 0:\n",
    "        ref_rank, out_rank = get_binned_values([v for (k, v) in sorted_ref], [v for (k, v) in sorted_out], bins)\n",
    "    else:\n",
    "        ref_rank = scipy.stats.rankdata([v for (k, v) in sorted_ref])\n",
    "        out_rank = scipy.stats.rankdata([v for (k, v) in sorted_out])\n",
    "\n",
    "    ref_rank_dict = {sorted_ref[i][0]: ref_rank[i] for i in xrange(len(ref_rank))}\n",
    "    out_rank_dict = {sorted_out[i][0]: out_rank[i] for i in xrange(len(out_rank))}\n",
    "\n",
    "    spearman_top = []\n",
    "    tau_top = []\n",
    "\n",
    "    for k in k_range:\n",
    "        top_elements = set([i[0] for i in sorted_ref[:k]]+[i[0] for i in sorted_out[:k]])\n",
    "        top_ref = [ref_rank_dict[i] for i in top_elements]\n",
    "        top_out = [out_rank_dict[i] for i in top_elements]\n",
    "        tau_top.append(scipy.stats.kendalltau(top_ref, top_out)[0])\n",
    "        spearman_top.append(scipy.stats.spearmanr(top_ref, top_out)[0])\n",
    "    return tau_top, spearman_top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ordered_pr_ref_basic, ordered_pr_out_basic = get_ordered_ranks(pr_basic, RS4_basic)\n",
    "#ordered_pr_ref_nopers, ordered_pr_out_nopers = get_ordered_ranks(pr_nopers, RS4_nopers)\n",
    "#ordered_pr_ref_rand, ordered_pr_out_rand = get_ordered_ranks(pr_rand, RS4_rand)\n",
    "    \n",
    "plt.rcParams.update({'font.size': 10, 'lines.linewidth': 3})\n",
    "plt.rcParams['xtick.labelsize'] = 15\n",
    "plt.rcParams['ytick.labelsize'] = 15\n",
    "\n",
    "mode = 'facebook'\n",
    "        \n",
    "# scatter plot for rankings\n",
    "plt.figure('scatter_' + mode)\n",
    "plt.scatter(ordered_pr_ref_basic, ordered_pr_out_basic, s=50, c='b')\n",
    "plt.plot(range(1, 101), range(1, 101), 'k--')\n",
    "plt.xlabel('static PageRank', fontsize=25)\n",
    "plt.ylabel('temporal PageRank', fontsize=25)\n",
    "plt.ylim((0, 101))\n",
    "plt.xlim((0, 101))\n",
    "plt.tight_layout()\n",
    "#plt.savefig(mode + '_scatter.pdf')\n",
    "plt.show()\n",
    "        \n",
    "        \n",
    "plt.figure('convergence_basic_' + mode)        \n",
    "plt.plot(x, pearson_basic, 'k--')\n",
    "plt.plot(x, spearman_basic, 'k-')\n",
    "#plt.plot(x, tau_basic, 'k-.')\n",
    "plt.plot(x, error_basic, 'k:')\n",
    "plt.xlabel('number of temporal edges', fontsize=25)\n",
    "plt.ylim((0, 1))\n",
    "plt.legend(['Pearson\\'s r', 'Spearman\\'s rho', 'error'], loc=0)\n",
    "plt.tight_layout()\n",
    "#plt.savefig(mode+'_convergence_basic.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:python3]",
   "language": "python",
   "name": "conda-env-python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
